{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbc6337",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "from math import erf, sqrt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "from scipy.stats import norm\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "#from random_forest import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4da128a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54feec3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df= pd.read_stata(\"bl_el_clean.dta\")\n",
    "df_main_old = pd.read_stata(\"new_cleaned het data.dta\")\n",
    "df_main = pd.read_stata(\"cleaned het data 2.dta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd29428",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3130785b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main['_merge'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1337ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363dbd6b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f84d32",
   "metadata": {},
   "source": [
    "# Calculating p(Z) for each household/row"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18854ded",
   "metadata": {},
   "source": [
    "#### First identifying the columns that we will use (columns in the new DF but not in the old one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e184ed8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main_old_col_names = set(df_main_old.columns)\n",
    "\n",
    "df_main_col_names = set(df_main.columns)\n",
    "\n",
    "\n",
    "exlusive_col_names = df_main_col_names- df_main_old_col_names\n",
    "\n",
    "\n",
    "df_main_exclusive = df_main.loc[:,list(exlusive_col_names)]\n",
    "df_main_exclusive.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54d0e11",
   "metadata": {},
   "source": [
    "##### Combining resticted and full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55baf83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main['full'] = df_main['full'] + df_main['restricted']\n",
    "df_main.drop(columns=['restricted'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f93b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main_old_col_names = set(df_main_old.columns)\n",
    "\n",
    "df_main_col_names = set(df_main.columns)\n",
    "\n",
    "\n",
    "exlusive_col_names = df_main_col_names- df_main_old_col_names\n",
    "\n",
    "\n",
    "df_main_exclusive = df_main.loc[:,list(exlusive_col_names)]\n",
    "df_main_exclusive.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0d49a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_main[df_main['full'] ==0].shape[0])\n",
    "print(df_main[df_main['half'] ==1].shape[0])\n",
    "print(df_main[df_main['control'] ==0].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c801ce73",
   "metadata": {},
   "source": [
    "### Combining the full cost and full cost restricted into one full cost variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b0fe03",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main['treatment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1caa3b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main['treatment'] = np.where(df_main['treatment'].isin(['Full cost','Full cost, restricted']), 'Full cost',df_main['treatment'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fd8a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main['treatment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173266e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "935 + 707"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2248c7",
   "metadata": {},
   "source": [
    "### Calculating p(Z) based on whether the value is in the Full cost, Half cost, or Control group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6db486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate p(Z) for full vs control\n",
    "F = df_main['full'].astype(float)     \n",
    "C = df_main['control'].astype(float)\n",
    "mask_F_vs_C = df_main['treatment'].isin(['Full cost','Control'])\n",
    "\n",
    "df_main['p(Z)_full_vs_control'] = np.nan\n",
    "df_main.loc[(mask_F_vs_C) & (df_main['treatment'].eq('Full cost')), 'p(Z)_full_vs_control'] = (F/(F+C)) # This says for rows in the mask (Full cost, Control), for rows where treatment equals Full cost, set the value of p(Z)_full_vs_control to (F/(F+C))\n",
    "df_main.loc[(mask_F_vs_C) & (df_main['treatment'].eq('Control')),   'p(Z)_full_vs_control'] = (C/(F+C))\n",
    "\n",
    "# calculate p(Z) for Hald vs Control\n",
    "H = df_main['half'].astype(float)     \n",
    "mask_H_vs_C = df_main['treatment'].isin(['Half cost','Control'])\n",
    "\n",
    "df_main['p(Z)_half_vs_control'] = np.nan\n",
    "df_main.loc[(mask_H_vs_C) & (df_main['treatment'].eq('Half cost')), 'p(Z)_half_vs_control'] = (H/(H+C))\n",
    "df_main.loc[(mask_H_vs_C) & (df_main['treatment'].eq('Control')),   'p(Z)_half_vs_control'] = (C/(H+C))\n",
    "\n",
    "# calculate p(Z) for Full vs Half\n",
    "mask_F_vs_H = df_main['treatment'].isin(['Full cost','Half cost'])\n",
    "\n",
    "df_main['p(Z)_full_vs_half'] = np.nan\n",
    "df_main.loc[(mask_F_vs_H) & (df_main['treatment'].eq('Full cost')), 'p(Z)_full_vs_half'] = (F/(F+H))\n",
    "df_main.loc[(mask_F_vs_H) & (df_main['treatment'].eq('Half cost')),   'p(Z)_full_vs_half'] = (H/(F+H))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29c6fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_main[df_main['treatment'].isin(['Full cost','Control'])].shape[0])\n",
    "print(df_main['p(Z)_full_vs_control'].notna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d63217",
   "metadata": {},
   "source": [
    "- Now when we filter by the population inside the function, we will get a p(Z) column for that population. \n",
    "- Note that although the p(Z) columns have missing, but after we filter by the population, we will get a p(Z) fir that population with no missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab028a49",
   "metadata": {},
   "source": [
    "#### Dropping the new not unneeded columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c1bd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main.drop(columns=['half_full','control_trt','full','half','total_n','_merge','control'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10899035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of all p(Z) variables\n",
    "p_z_list = ['p(Z)_full_vs_control','p(Z)_half_vs_control','p(Z)_full_vs_half']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed050768",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4eaafe",
   "metadata": {},
   "source": [
    "# Transforming the column that will be used for CLAN:\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "clan_cols = [\n",
    "    \"treated_HHs\",\"share_all\",\"small_ls_bl\",\"medium_ls_bl\",\"large_ls_bl\",\"w05_ls_value\",\n",
    "    \"fattening_ls_count\",\"sold_ls_count\",\"consumed_ls_count\",\"dwelling_rent\",\"water_source_dist\",\n",
    "    \"hh_share_latrine\",\"home_improve_amount\",\"ratio_smart_mobil_16\",\"ratio_skiptimes_p_17\",\n",
    "    \"ratio_eatoutside__24\",\"nonfood_exp_mon_bl\",\"food_exp_bl\",\"nonagri_sumcapital\",\n",
    "    \"agri_own_size_quir\",\"agri_rent_size_quir\",\"agri_rent_amount_bl\",\"agri_rent_out_siz_28\",\n",
    "    \"agri_rent_out_amount\",\"bl_fruit_trees_count\",\"totalincome_witho_32\",\"totalincome_1mo\",\n",
    "    \"propdisabled_ill\",\"use_time_rev_30da_35\",\"use_time_edu_30da_36\",\"health_cant_work__61\",\n",
    "    \"genderrole_mean\",\"hh_prop_prepplus\",\"hh_prop_enrolled\",\"hh_max_attain\",\"hh_size\",\n",
    "    \"hh_n_work\",\"dep_ratio\",\"hh_female_prop\",\"own_value_ls\",\"log_totalincome_1mo\",\n",
    "    \"elset_sh_eq\",\"inflated_ls_hh\",\"tot_exp_2\"\n",
    "]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "df[[f\"{c}_std\" for c in clan_cols]] = scaler.fit_transform(df[clan_cols]).copy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63558a89",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517b8e15",
   "metadata": {},
   "source": [
    "## Y values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23985da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing the amout that we given to the treatment group from their reported values\n",
    "df_main['own_value_ls'] = np.where(df_main['treatment'] == 1,df_main['own_value_ls'] - 11379.31, df_main['own_value_ls'] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54aac51",
   "metadata": {},
   "source": [
    "##### **inflated_ls_hh is a Y variable and only has 3 unique float values?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2dbb734",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_main['inflated_ls_hh'].nunique())\n",
    "df_main['inflated_ls_hh'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e84163",
   "metadata": {},
   "source": [
    "#### totalincome_1mo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220f22f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_main['totalincome_1mo'].nunique())\n",
    "print(df_main['totalincome_1mo'].value_counts())\n",
    "print(f\"% of non-0 values: {df_main[df_main['totalincome_1mo'] !=0].shape[0] / df_main.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a55d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_main.shape[0])\n",
    "print(df_main['totalincome_1mo'].nunique())\n",
    "df_main['totalincome_1mo'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8ed8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df_main['totalincome_1mo'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e629145",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_main['own_value_ls'].nunique())\n",
    "print(df_main['own_value_ls'].value_counts())\n",
    "print(f\"% of non-0 values: {df_main[df_main['own_value_ls'] !=0].shape[0] / df_main.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69634348",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df_main['own_value_ls'],bins=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7091e33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_main['tot_exp_2'].nunique())\n",
    "print(df_main['tot_exp_2'].value_counts().sort_values())\n",
    "#print(f\"% of non-0 values: {df_main[df_main['tot_exp_2'] !=0].shape[0] / df_main.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ae6fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df_main['tot_exp_2'],bins=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6996ff95",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_main['elset_sh_eq'].nunique())\n",
    "print(df_main['elset_sh_eq'].value_counts().sort_values(ascending=False))\n",
    "print(f\"% of non-0 values: {df_main[df_main['elset_sh_eq'] !=0].shape[0] / df_main.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65cbf153",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df_main['elset_sh_eq'],bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cb3d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_main['elset_sh_eq'] = np.log1p(df_main['elset_sh_eq'])\n",
    "#df_main['totalincome_1mo'] = np.log1p(df_main['totalincome_1mo'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc17ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_main['own_value_ls'] = np.log1p(df_main['own_value_ls'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1749c8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main['own_value_ls'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a812834b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main['totalincome_1mo'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a524684",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main['own_value_ls'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a3a554",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main[df_main['elset_sh_eq'] > 30].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065ff51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main['elset_sh_eq'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58cfe710",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main[df_main['elset_sh_eq'] > 30].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f4544a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main[(df_main['elset_sh_eq'].rank(pct=True) >= 0.90) & (df_main['elset_sh_eq']>=100)].shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af4191b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{df_main[df_main['elset_sh_eq'].rank(pct=True).between(0.95, 1)]['own_value_ls'].mean()}\")\n",
    "print(f\"{df_main[df_main['elset_sh_eq'].rank(pct=True).between(0.95, 1)]['own_value_ls'].median()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1ef9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main['elset_sh_eq'].quantile(0.95) #filter by 95th percentile only for this variable run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11da600",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main['elset_sh_eq'].quantile(0.95) / df_main['elset_sh_eq'].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb17a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df_main['elset_sh_eq'],bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8466ce89",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_y_list = [\"own_value_ls\",\"totalincome_1mo\",\"elset_sh_eq\",\"tot_exp_2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c088df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log of the y, Experimental:\n",
    "#all_y_list = [\"own_value_ls\",\"totalincome_1mo\",\"elset_sh_eq\",\"tot_exp_2\",\"inflated_ls_hh\"]\n",
    "\n",
    "#df_main['totalincome_1mo'] = np.log1p(df_main['totalincome_1mo'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380b3557",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f5f03b",
   "metadata": {},
   "source": [
    "### Cleaning ID and other irrelevant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a85ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chcecking all cols types\n",
    "unique_dtypes = df_main.dtypes.map(lambda dt: dt.name).unique()\n",
    "print(unique_dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f110891",
   "metadata": {},
   "outputs": [],
   "source": [
    "object_columns = df_main.select_dtypes(include=['object']).columns\n",
    "object_columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097c52e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main.drop(columns=['hhid', 'hhid_id', 'agg_id', 'q_id','nn','inflated_ls_hh'], inplace=True)\n",
    "object_columns = df_main.select_dtypes(include=['object']).columns\n",
    "object_columns "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515dbf62",
   "metadata": {},
   "source": [
    "## Categorical Variables Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a077e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main['ladder'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7424ff43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Drop the row(s) with ladder == 2.077067 \n",
    "df_main = df_main[~df_main[\"ladder\"].astype(str).str.strip().eq(\"2.077067\")].copy()\n",
    "\n",
    "# 2) Convert labels like \"1 - extremly poor living\" and \"10 - very comfortable living\" to integers\n",
    "df_main[\"ladder\"] = df_main[\"ladder\"].astype(str).str.extract(r\"^\\s*(\\d+)\")[0].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dae3ae1",
   "metadata": {},
   "source": [
    "#### Converting the binary columns from string to int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24bdcee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0f78df",
   "metadata": {},
   "outputs": [],
   "source": [
    "yes_no_cols = ['own_ls','ls_stable','latrine','worry_about_food','got_medical_assist',\n",
    "               'agri_own_yesno', 'agri_rent_yesno', 'agri_rent_out_yesno', 'hhmembers_agri_work', \n",
    "               'cart_e', 'bicycle_e', 'vehicle_toktok_e', 'vehicle_tricycle_e', 'vehicle_motorcycle_e', \n",
    "               'has_fridge_e','skip_meal_e','has_heater_e']\n",
    "\n",
    "for col in yes_no_cols:\n",
    "    df_main[col] = np.where(df_main[col] == 'Yes', 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26997ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a5058b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_main['haschld'].value_counts())\n",
    "print()\n",
    "print(df_main['illithd'].value_counts())\n",
    "print()\n",
    "print(df_main['hhdisabl_chld'].value_counts())\n",
    "print()\n",
    "print(df_main['hhdisabl_adult'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c0f408",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main['femhd'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adcbf944",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_string_cols = ['haschld', 'illithd', 'hhdisabl_chld', 'hhdisabl_adult','oldhd','younghd','singhd']\n",
    "\n",
    "for col in binary_string_cols:\n",
    "    df_main[col] = (pd.to_numeric(df_main[col], errors=\"coerce\") != 0).astype(\"int8\")\n",
    "\n",
    "df_main['femhd'] = np.where(df_main['femhd'] == 'female HH-head', 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecc8d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_main['haschld'].value_counts())\n",
    "print()\n",
    "print(df_main['illithd'].value_counts())\n",
    "print()\n",
    "print(df_main['hhdisabl_chld'].value_counts())\n",
    "print()\n",
    "print(df_main['hhdisabl_adult'].value_counts())\n",
    "print()\n",
    "print(df_main['oldhd'].value_counts())\n",
    "print()\n",
    "print(df_main['younghd'].value_counts())\n",
    "print()\n",
    "print(df_main['singhd'].value_counts())\n",
    "print()\n",
    "print(df_main['femhd'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d8ee21",
   "metadata": {},
   "source": [
    "### Clean some binary columns to int instead of float"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663b7dcb",
   "metadata": {},
   "source": [
    "#### Ask about this variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ec71f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ff598a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns with exactly two unique values\n",
    "binary_cols = [c for c in df_main.columns if df_main[c].nunique() == 2]\n",
    "\n",
    "for col in binary_cols:\n",
    "    df_main[col] = df_main[col].astype(\"Int64\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9c506b",
   "metadata": {},
   "source": [
    "### Defining a list of all potential target variables to be used as a parameter in the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bdcc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_y_list = [\"own_value_ls\",\"totalincome_1mo\",\"elset_sh_eq\",\"tot_exp_2\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b636ef9",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5082dc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main['treatment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f7e26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of all p(Z) variables\n",
    "p_z_list = ['p(Z)_full_vs_control','p(Z)_half_vs_control','p(Z)_full_vs_half']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79697018",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_heterogeneity(df_main, y, population_1, population_2, n_splits, ml_model, binary_cols, all_y_list):\n",
    "    \"\"\"\n",
    "    df: pandas.DataFrame\n",
    "    y: str  (e.g., \"own_value_ls\")\n",
    "    population_1, population_2: values in df['treatment'] to compare (pop1=1, pop2=0)\n",
    "    n_splits: int\n",
    "    ml_model: str or estimator (e.g., \"catboost\", \"xgboost\", \"lightgbm\", \"rf\", or a fitted-like object)\n",
    "        Currently we have \"catboost\", \"xgboost\", \"nn\" options.\n",
    "    all_y_list: list of strings that contains all the options for target variables, after passing the wanted target variable, the rest will be dropped \n",
    "    \"\"\"\n",
    "\n",
    "    #p_of_z = 1.0/2.0 #Delete later\n",
    "    binary_cols = binary_cols\n",
    "\n",
    "    df_population = df_main.copy() \n",
    "    # Defining the DataFrame:\n",
    "    df_population = df_population[df_population[\"treatment\"].isin([population_1, population_2])].copy()\n",
    "    df_population[\"D\"] = np.where(df_population[\"treatment\"] == population_1, 1, 0)  # pop1=1, pop2=0\n",
    "\n",
    "    # Defining p(Z):\n",
    "    if {population_1, population_2} == {\"Full cost\", \"Control\"}:\n",
    "        df_population.drop(columns=['p(Z)_half_vs_control','p(Z)_full_vs_half'], inplace=True)\n",
    "        df_population[\"p(Z)\"] = df_population['p(Z)_full_vs_control']\n",
    "\n",
    "    elif {population_1, population_2} == {\"Half cost\", \"Control\"}:\n",
    "        df_population.drop(columns=['p(Z)_full_vs_control','p(Z)_full_vs_half'], inplace=True)\n",
    "        df_population[\"p(Z)\"] = df_population['p(Z)_half_vs_control']\n",
    "\n",
    "    elif {population_1, population_2} == {\"Full cost\", \"Half cost\"}:\n",
    "        df_population.drop(columns=['p(Z)_full_vs_control','p(Z)_half_vs_control'], inplace=True)\n",
    "        df_population[\"p(Z)\"] = df_population['p(Z)_full_vs_half']\n",
    "\n",
    "\n",
    "    # Dropping the treatment column:\n",
    "    df_population.drop(columns=[\"treatment\"], inplace=True)\n",
    "\n",
    "\n",
    "    # Results container:\n",
    "    results = {\"blp\": [], \"gates\": [], \"clan\": []}\n",
    "    models_comparisons = {\"small_lambda_blp\":[], \"small_lambda_gates\":[]}  # to store the models comparison results. BLP and GATES small Lambdas\n",
    "\n",
    "    \n",
    "    # Dropping the columns that are not features:\n",
    "    # list of all Y columns\n",
    "    # Option A: copy + remove\n",
    "    all_y_list = all_y_list.copy()\n",
    "    others_y_list = all_y_list.copy()\n",
    "    others_y_list.remove(y)              # y must be in the list\n",
    "    df_population = df_population.drop(columns=others_y_list)\n",
    "\n",
    "    # Creating a dictionary to store ML proxy predictions\n",
    "    s_records = {idx: [] for idx in df_population.index} #dict with keys as the idx, and for each one, we have an empty list to each loop's prediction per idx/row.\n",
    "\n",
    "    #-------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    # Big loop:\n",
    "    base_seed = 7\n",
    "    for split in range(n_splits):\n",
    "        df = df_population.copy() # Reformating the original DF for each split so the split specific columns are re-defined\n",
    "\n",
    "        # D-stratified 50/50 split \u2192 persistent 'fold' column\n",
    "        # first assign all values to main\n",
    "        df[\"fold\"] = \"main\" \n",
    "        # Then assign half of each group of D (0 & 1) to aux randomly.\n",
    "        aux_idx = (df.groupby(\"D\", group_keys=False)\n",
    "               .apply(lambda g: g.sample(frac=0.5, random_state= base_seed + split*10))).index   # group_keys=False\n",
    "        df.loc[aux_idx, \"fold\"] = \"aux\"\n",
    "\n",
    "    #-------------------------------------------------------------------------------------------------------------------\n",
    "        # ML Proxy:\n",
    "        # Defining the data for the two ML models:\n",
    "        u_0_data = df[(df['fold'] == 'aux') & (df['D'] == 0)].copy()\n",
    "        u_1_data = df[(df['fold'] == 'aux') & (df['D'] == 1)].copy()\n",
    "\n",
    "        # 1) Define X, y for each aux subset\n",
    "        drop_cols = [y, \"D\", \"fold\", \"p(Z)\"]\n",
    "\n",
    "        X_0 = u_0_data.drop(columns=drop_cols)\n",
    "        y_0 = u_0_data[y]\n",
    "\n",
    "        X_1 = u_1_data.drop(columns=drop_cols)\n",
    "        y_1 = u_1_data[y]\n",
    "\n",
    "        # Decision the ML model to use\n",
    "       # if ml_model == 'catboost':\n",
    "        #    alpha = 0.45\n",
    "        #    cb_params = dict(\n",
    "        #        iterations=700,\n",
    "        #        depth=8,\n",
    "        #        learning_rate=0.06,\n",
    "        #        loss_function=f\"Quantile:alpha={alpha}\",\n",
    "        #        #loss_function=\"Huber:delta=2000\",\n",
    "        #        eval_metric=f\"SMAPE\",           # robust eval\n",
    "        #        bootstrap_type=\"Bernoulli\",\n",
    "        #        subsample=0.8,\n",
    "        #        rsm=0.8,\n",
    "        #        l2_leaf_reg=6.0,\n",
    "        #        od_type=\"Iter\", od_wait=60,     # early stopping\n",
    "        #        random_seed=39,\n",
    "        #        verbose=False,\n",
    "        #        allow_writing_files=False,\n",
    "        #    )\n",
    "\n",
    "\n",
    "#-----------------------------------------------------------------------\n",
    "\n",
    "        if ml_model == 'catboost': \n",
    "            #alpha = 0.9\n",
    "            cb_params = dict(\n",
    "                iterations= 700, \n",
    "                depth= 8, \n",
    "                learning_rate= 0.05, \n",
    "               # loss_function= f\"Quantile:alpha={alpha}\", \n",
    "                #loss_function = 'RMSE',\n",
    "                loss_function = 'Huber:delta=1000',\n",
    "                #eval_metric= f\"SMAPE\", \n",
    "                #robust eval bootstrap_type=\"Bernoulli\",\n",
    "                #subsample=0.8, rsm=0.8, l2_leaf_reg=6.0,\n",
    "                #od_type=\"Iter\",\n",
    "                #od_wait=60,\n",
    "                # early stopping random_seed=39,\n",
    "                verbose=False,\n",
    "                # allow_writing_files=False,\n",
    "\n",
    "        )\n",
    "\n",
    "            # fit\n",
    "            model_u1 = CatBoostRegressor(**cb_params).fit(Pool(X_1, y_1, cat_features=binary_cols))\n",
    "            model_u0 = CatBoostRegressor(**cb_params).fit(Pool(X_0, y_0, cat_features=binary_cols))\n",
    "\n",
    "        elif ml_model == 'xgboost':\n",
    "            xgb_params = dict(\n",
    "                n_estimators=700,\n",
    "                max_depth=8,\n",
    "                learning_rate=0.1,\n",
    "                #subsample=0.8,\n",
    "                #colsample_bytree=0.8,\n",
    "                #random_state=39,\n",
    "                #n_jobs=-1,\n",
    "                objective=\"reg:squarederror\",\n",
    "                #enable_categorical=True\n",
    "            )\n",
    "\n",
    "            model_u1 = XGBRegressor(**xgb_params).fit(X_1, y_1)  # \u03bc\u03021(Z)\n",
    "            model_u0 = XGBRegressor(**xgb_params).fit(X_0, y_0)  # \u03bc\u03020(Z)\n",
    "            \n",
    "        # Creating a RF option\n",
    "        elif ml_model == 'rf':\n",
    "            rf_params = dict(\n",
    "                n_estimators=300,\n",
    "                max_depth=8,\n",
    "                max_features=\"sqrt\",   # similar to colsample_bytree\n",
    "                min_samples_leaf=1, # minimum number of samples required to be in a leaf node\n",
    "                #n_jobs=-1, # ontrols how many CPU cores scikit-learn will use in parallel (n_jobs=-1: use all available cores.)\n",
    "                #random_state=39\n",
    "            )\n",
    "\n",
    "            model_u1 = RandomForestRegressor(**rf_params).fit(X_1, y_1)  # \u03bc\u03021(Z)\n",
    "            model_u0 = RandomForestRegressor(**rf_params).fit(X_0, y_0)  # \u03bc\u03020(Z)\n",
    "\n",
    "        # Neural Network Option:\n",
    "        elif ml_model == 'nn':\n",
    "            # to float32\n",
    "            X1 = np.asarray(X_1, dtype=\"float32\"); y1 = np.asarray(y_1, dtype=\"float32\")\n",
    "            X0 = np.asarray(X_0, dtype=\"float32\"); y0 = np.asarray(y_0, dtype=\"float32\")\n",
    "\n",
    "            model_u1 = keras.Sequential([\n",
    "                layers.Input(shape=(X1.shape[1],)),\n",
    "                layers.Dense(256, activation=\"relu\", kernel_regularizer=regularizers.l2(1e-4)),\n",
    "                #layers.BatchNormalization(),\n",
    "                layers.Dense(64, activation=\"relu\", kernel_regularizer=regularizers.l2(1e-4)),\n",
    "                #layers.BatchNormalization(),\n",
    "                layers.Dense(32, activation=\"relu\", kernel_regularizer=regularizers.l2(1e-4)),\n",
    "                #layers.BatchNormalization(),\n",
    "                layers.Dense(16, activation=\"relu\", kernel_regularizer=regularizers.l2(1e-4)),\n",
    "                #layers.BatchNormalization(),\n",
    "                layers.Dense(1)\n",
    "                ])\n",
    "            model_u1.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-3, clipnorm=1.0),\n",
    "                            loss=\"mse\",\n",
    "                            metrics=[keras.metrics.RootMeanSquaredError()]\n",
    "                            )\n",
    "\n",
    "            model_u0 = keras.Sequential([\n",
    "                layers.Input(shape=(X0.shape[1],)),\n",
    "                layers.Dense(256, activation=\"relu\", kernel_regularizer=regularizers.l2(1e-4)),\n",
    "                #layers.BatchNormalization(),\n",
    "                layers.Dense(64, activation=\"relu\", kernel_regularizer=regularizers.l2(1e-4)),\n",
    "                #layers.BatchNormalization(),\n",
    "                layers.Dense(32, activation=\"relu\", kernel_regularizer=regularizers.l2(1e-4)),\n",
    "                #layers.BatchNormalization(),\n",
    "                layers.Dense(16, activation=\"relu\", kernel_regularizer=regularizers.l2(1e-4)),\n",
    "                #layers.BatchNormalization(),\n",
    "                layers.Dense(1)\n",
    "            ])\n",
    "            model_u0.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-3, clipnorm=1.0),\n",
    "                            loss=\"mse\",\n",
    "                            metrics=[keras.metrics.RootMeanSquaredError()]\n",
    "                            )\n",
    "\n",
    "            # fit  (keep variables as the MODEL, not the History)\n",
    "            model_u1.fit(X1, y1, epochs=50, batch_size=64, verbose=0)\n",
    "            model_u0.fit(X0, y0, epochs=50, batch_size=64, verbose=0)\n",
    "\n",
    "            ### Trying an Elastic Net Model option:\n",
    "            # Elastic Net option:\n",
    "        elif ml_model == 'elasticnet':\n",
    "            enet_params = dict(\n",
    "                l1_ratio=[0.1, 0.3, 0.5, 0.7, 0.9],  # search over mix of L1/L2\n",
    "                n_alphas=100,\n",
    "                cv=5,\n",
    "                random_state=39,\n",
    "                max_iter=5000,\n",
    "                n_jobs=-1\n",
    "            )\n",
    "            model_u1 = Pipeline([\n",
    "                (\"scaler\", StandardScaler()),\n",
    "                (\"enet\", ElasticNetCV(**enet_params))\n",
    "            ]).fit(X_1, y_1)\n",
    "            model_u0 = Pipeline([\n",
    "                (\"scaler\", StandardScaler()),\n",
    "                (\"enet\", ElasticNetCV(**enet_params))\n",
    "            ]).fit(X_0, y_0)\n",
    "\n",
    "\n",
    "        #elif ml_model == 'xyz': Place holder to add other model options later.\n",
    "\n",
    "        #------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "        # drop list you used when training\n",
    "        drop_cols = [y, \"D\", \"fold\", \"p(Z)\"]\n",
    "\n",
    "        # Applying the predictions to the main fold:\n",
    "        # main fold\n",
    "        main = df[df[\"fold\"] == \"main\"].copy()\n",
    "\n",
    "        # features for main (must match training: drop non-features)\n",
    "        X_main = main.drop(columns=drop_cols).copy()\n",
    "\n",
    "\n",
    "        if ml_model == 'nn':\n",
    "            X_pred = X_main.reindex(columns=X_1.columns, fill_value=0).to_numpy(dtype=\"float32\")\n",
    "            model_u1_hat = np.asarray(model_u1.predict(X_pred)).reshape(-1)\n",
    "            model_u0_hat = np.asarray(model_u0.predict(X_pred)).reshape(-1)\n",
    "        else:\n",
    "            # predict \u03bc\u03021 and \u03bc\u03020 on main, then S = \u03bc\u03021 \u2212 \u03bc\u03020 on any other model\n",
    "            model_u1_hat = model_u1.predict(X_main)\n",
    "            model_u0_hat = model_u0.predict(X_main)\n",
    "\n",
    "        # we filter df by the location of the X_main, and then create a new column\n",
    "        df.loc[main.index, \"S(Z)\"] = model_u1_hat - model_u0_hat\n",
    "\n",
    "        # Also adding the base line preidictions B(Z):\n",
    "        df.loc[main.index, \"B(Z)\"] = model_u0_hat\n",
    "\n",
    "\n",
    "        # Collect S(Z) for rows that are in 'main' this split\n",
    "        for idx, sval in df.loc[main.index, \"S(Z)\"].items():\n",
    "            if pd.notnull(sval):\n",
    "                s_records[idx].append(float(sval))\n",
    "\n",
    "        #---------------------------------------------------------------------------------------------\n",
    "        # BLP:\n",
    "\n",
    "        # First Creating some BLP equation columns:\n",
    "        # Creating the D-p column:\n",
    "        mask = df['fold'].eq('main')\n",
    "        df['D-p'] = np.nan\n",
    "        #df.loc[mask, 'D-p'] = df.loc[mask, 'D'].astype(float) - float(p_of_z) \n",
    "        df.loc[mask, 'D-p'] = df.loc[mask, 'D'].astype(float) - df.loc[mask, 'p(Z)'].astype(float)\n",
    "\n",
    "\n",
    "        # Create the shifted (centered) ML proxy on the MAIN sample\n",
    "        mask = df['fold'].eq('main')\n",
    "        df['S(Z)_shifted'] = np.nan\n",
    "        mu_S = df.loc[mask, 'S(Z)'].mean()\n",
    "        df.loc[mask, 'S(Z)_shifted'] = df.loc[mask, 'S(Z)'] - mu_S\n",
    "\n",
    "        # Creating a single (D-p)*shifted_S_xgb column\n",
    "        mask = df['fold'].eq('main')\n",
    "        df['D-p_S(Z)_shifted'] = np.nan\n",
    "        df.loc[mask, 'D-p_S(Z)_shifted'] = df.loc[mask, 'D-p'].astype(float) * df.loc[mask, 'S(Z)_shifted'].astype(float)\n",
    "\n",
    "\n",
    "        # BLP Model:\n",
    "        \n",
    "        mask = df['fold'].eq('main')\n",
    "\n",
    "        y_vec = df.loc[mask, y]  \n",
    "        X = df.loc[mask, ['D-p', 'D-p_S(Z)_shifted', 'B(Z)', 'p(Z)']]\n",
    "\n",
    "        X = sm.add_constant(X)  # adds the intercept \u03b1\n",
    "\n",
    "        blp = sm.OLS(y_vec, X).fit(cov_type='HC1')  # robust SEs\n",
    "\n",
    "        # Saving the BLP results:\n",
    "        # We will save the R-Squared, R-Squared Adjusted, p-value, and the coefficients for D-p and  D-p_S(Z)_shifted.\n",
    "        # column keys must match exactly how you named them in X\n",
    "\n",
    "\n",
    "        beta2 = blp.params['D-p_S(Z)_shifted']      # slope on (D-p) * (S - mean_S)\n",
    "        S_main = df.loc[mask, 'S(Z)'].to_numpy()    # raw S on the main split (not standardized)\n",
    "        #var_y_main = float(np.var(df.loc[mask,y], ddof=0)) # population var to divide by\n",
    "\n",
    "        small_lambda_blp = float(beta2**2) * float(np.var(S_main, ddof=0)) \n",
    "\n",
    "        #small_lambda_blp = float(beta2**2) * float(np.var(S_main, ddof=0))  / var_y_main\n",
    "\n",
    "\n",
    "        # inside the loop, AFTER fitting `blp`\n",
    "        k1, k2 = \"D-p\", \"D-p_S(Z)_shifted\"\n",
    "\n",
    "        def grab(key):\n",
    "            lo, hi = blp.conf_int().loc[key]\n",
    "            return {\n",
    "                \"name\": key,\n",
    "                \"coef\": blp.params[key],\n",
    "                \"std_err\": blp.bse[key],\n",
    "                \"z\": blp.tvalues[key],\n",
    "                \"p\": blp.pvalues[key],\n",
    "                \"ci_low\": lo,\n",
    "                \"ci_high\": hi,\n",
    "            }\n",
    "\n",
    "        split_blp = [\n",
    "            {\"name\": \"D_minus_p\", **grab(k1)},\n",
    "            {\"name\": \"D_minus_p_times_S\", **grab(k2)},\n",
    "        ]\n",
    "\n",
    "        beta2 = blp.params['D-p_S(Z)_shifted']      # slope on (D-p) * (S - mean_S)\n",
    "        S_main = df.loc[mask, 'S(Z)'].to_numpy()    # raw S on the main split (not standardized)\n",
    "\n",
    "        #small_lambda_blp = float(beta2**2) * float(np.var(S_main, ddof=0))  # population var\n",
    "        small_lambda_blp = float(beta2**2) * float(np.var(S_main, ddof=1))\n",
    "\n",
    "\n",
    "        results[\"blp\"].append(split_blp)   \n",
    "        models_comparisons[\"small_lambda_blp\"].append(small_lambda_blp)\n",
    "        \n",
    "    #return results # Only used for debugging\n",
    "        \n",
    "\n",
    "       #-----------------------------------------------------------------------------------------------\n",
    "       # GATES\n",
    "\n",
    "       # We will start by creating a new column that assigns quantile groups to each S(Z) value.\n",
    "       # MAIN-only groups for GATES\n",
    "        mask_main = df[\"fold\"].eq(\"main\")\n",
    "        s_main = df.loc[mask_main, \"S(Z)\"].dropna()\n",
    "\n",
    "        df[\"Group\"] = np.nan\n",
    "        df.loc[mask_main, \"Group\"] = (\n",
    "            pd.qcut(s_main, q=5, labels=[1,2,3,4,5], duplicates=\"drop\")\n",
    "            .astype(float)\n",
    "        )\n",
    "\n",
    "        # Creating the main column for the GATES model, which is T_K\n",
    "        # T_k = (D - p) * 1{Group=k}\n",
    "        for k in range(1, 6):\n",
    "            df.loc[mask_main, f\"T{k}\"] = (\n",
    "                df.loc[mask_main, \"D-p\"] *\n",
    "                (df.loc[mask_main, \"Group\"].astype(int) == k).astype(int)\n",
    "            )\n",
    "\n",
    "\n",
    "        # Fit GATES Model:\n",
    "        dfm = df.loc[mask_main, [y, \"T1\",\"T2\",\"T3\",\"T4\",\"T5\",\"B(Z)\"]].dropna()\n",
    "        X = sm.add_constant(dfm[[\"T1\",\"T2\",\"T3\",\"T4\",\"T5\",\"B(Z)\"]], has_constant=\"add\")\n",
    "        gates = sm.OLS(dfm[y], X).fit(cov_type=\"HC1\")\n",
    "        params = gates.params\n",
    "\n",
    "\n",
    "        # Saving the GATES results in our results dictionary:\n",
    "        ci = gates.conf_int()  # 95% by default\n",
    "        params = gates.params\n",
    "        bse = gates.bse\n",
    "\n",
    "        split_gates = []\n",
    "        for k in range(1, 6):\n",
    "            tk = f\"T{k}\"\n",
    "            split_gates.append({\n",
    "                \"group\": k,\n",
    "                \"gamma\": float(params[tk]),\n",
    "                \"std_err\": float(bse[tk]),\n",
    "                \"ci_low\": float(ci.loc[tk, 0]),\n",
    "                \"ci_high\": float(ci.loc[tk, 1]),\n",
    "            })\n",
    "\n",
    "        results[\"gates\"].append(split_gates)\n",
    "\n",
    "        bar_lambda = 0\n",
    "        for k in range(1, 6):\n",
    "            tk = f\"T{k}\"\n",
    "            bar_lambda += float(gates.params[tk])**2 \n",
    "        \n",
    "        #bar_lambda_gates = (bar_lambda / 5.0) / var_y_main\n",
    "        bar_lambda_gates = (bar_lambda / 5.0) \n",
    "\n",
    "\n",
    "        models_comparisons[\"small_lambda_gates\"].append(bar_lambda_gates)\n",
    "\n",
    "        \n",
    "\n",
    "       #-----------------------------------------------------------------------------------------------\n",
    "       # CLAN\n",
    "        # Steps:\n",
    "            #1. Get only the numeric columns.\n",
    "            #2. Remove all the columns created in the previous steps and the target column.\n",
    "            #3. Remove all the columns that numericly coded but are binary.\n",
    "            #4. Remove additional columns that are categorical but were not detected as binary (found after in the CLAN table)\n",
    "            #5. Getting a list of all columns that will be entered to CLAN\n",
    "\n",
    "        # 1. Keeping only numeric columns\n",
    "        df_numeric = df.select_dtypes(include=[np.number])\n",
    "\n",
    "        # 2. Removing the columns created in the previous steps\n",
    "        clan_exclude = [\n",
    "            \"D\",\n",
    "            \"S(Z)\",\n",
    "            \"D-p\",\n",
    "            \"S(Z)_shifted\",\n",
    "            \"D-p_S(Z)_shifted\",\n",
    "            \"B(Z)\",\n",
    "            \"p(Z)\",\n",
    "            \"Group\",\n",
    "            \"T1\", \"T2\", \"T3\", \"T4\",\"T5\",\n",
    "            y\n",
    "        ]\n",
    "        # 3. Removing binary columns:\n",
    "        binary_cols_01 = [c for c in df_numeric.columns\n",
    "                        if set(df_numeric[c].dropna().unique()) == {0, 1}]\n",
    "\n",
    "        #4.Remove additional columns that are categorical but were not detected as binary (found after in the CLAN table)\n",
    "        additional_features_drop = ['vehicle_motorcycle_e','vehicle_toktok_e','vehicle_tricycle_e','skip_meal_e','has_heater_e','has_fridge_e','cart_e','bicycle_e']\n",
    "\n",
    "        #5. Getting a list of all columns that will be entered to CLAN\n",
    "        # Combing all exclude column lists and filter the numeric DF:\n",
    "        cols_to_remove = clan_exclude + binary_cols_01 + additional_features_drop\n",
    "\n",
    "        # filtering the numeric DF to get only a DF with only useful columns\n",
    "        df_clan_cols = df_numeric.drop(columns=cols_to_remove)\n",
    "\n",
    "        # This is our final feature list\n",
    "        clan_cols = df_clan_cols.columns\n",
    "\n",
    "        # Creating a Clan Table\n",
    "        # 1. Filter to inlcude only the main fold\n",
    "        # 2. Include only the CLAN columns + the group column\n",
    "        # 3. Include only group 1 and 5\n",
    "\n",
    "        # filtering to include only main fold\n",
    "        df_main = df[df['fold'] == 'main']\n",
    "\n",
    "        # Filtering to include only CLAN columns + group\n",
    "        df_clan = df_main.loc[:, list(clan_cols) + [\"Group\"]].copy()\n",
    "\n",
    "        # keep only groups 1 and 5 (fixing the var name)\n",
    "        df_clan = df_clan[df_clan[\"Group\"].isin([1, 5])]\n",
    "\n",
    "\n",
    "\n",
    "        #  Calculating the means, the diff, the SE, the z, the p, the CI for each variable\n",
    "        stats = (\n",
    "            df_clan.groupby(\"Group\")[clan_cols]\n",
    "            .agg([\"mean\", \"std\", \"count\"])\n",
    "            .reindex([1, 5])  # ensure order: G1, G5\n",
    "        )\n",
    "\n",
    "        rows = []\n",
    "        zcrit = 1.96  # 95% CI using normal critical value\n",
    "\n",
    "        for col in clan_cols:\n",
    "            m1 = stats.loc[1, (col, \"mean\")]\n",
    "            m5 = stats.loc[5, (col, \"mean\")]\n",
    "            s1 = stats.loc[1, (col, \"std\")]\n",
    "            s5 = stats.loc[5, (col, \"std\")]\n",
    "            n1 = stats.loc[1, (col, \"count\")]\n",
    "            n5 = stats.loc[5, (col, \"count\")]\n",
    "\n",
    "            diff = m5 - m1\n",
    "            \n",
    "            # Welch SE for difference in means\n",
    "            se = np.sqrt((s5**2) / n5 + (s1**2) / n1) \n",
    "            z  = diff / se\n",
    "\n",
    "            # Two-sided p-value from the normal CDF via erf\n",
    "            # Phi(z) = 0.5 * (1 + erf(z / sqrt(2)))\n",
    "            \n",
    "            p_two_sided = 2 * (1 - (0.5 * (1 + erf(abs(z) / sqrt(2)))))\n",
    "        \n",
    "\n",
    "            ci_lo = diff - zcrit * se \n",
    "            ci_hi = diff + zcrit * se \n",
    "\n",
    "            # save the results to our results dictionary\n",
    "            rows.append({\n",
    "                \"covariate\": col,\n",
    "                \"Mean G1 (predicted least affected)\": m1,\n",
    "                \"Mean G5 (predicted most affected)\": m5,\n",
    "                \"Diff (G5 - G1)\": diff,\n",
    "                \"SE (Welch)\": se,\n",
    "                \"z\": z,\n",
    "                \"p (two-sided)\": p_two_sided,\n",
    "                \"CI 95% lower\": ci_lo,\n",
    "                \"CI 95% upper\": ci_hi,\n",
    "            })\n",
    "\n",
    "        # save CLAN for this split (one big list of covariate rows)\n",
    "        results[\"clan\"].append(rows)\n",
    "\n",
    "        # End of the loop\n",
    "\n",
    "        # Calculating the median S(Z) for each row/list\n",
    "        s_median_series = pd.Series(\n",
    "            {idx: (np.median(vals) if len(vals) > 0 else np.nan) for idx, vals in s_records.items()}\n",
    "        ).reindex(df_population.index)\n",
    "\n",
    "        # If you specifically want a plain list:\n",
    "        s_medians_per_row = s_median_series.tolist()\n",
    "\n",
    "\n",
    "\n",
    "    #------------------------------------------------------------------------------------------------\n",
    "    return results, models_comparisons, s_medians_per_row # the final result dictionaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad707d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "results, models_comparisons, s_medians_per_row = estimate_heterogeneity(df_main, 'own_value_ls', 'Full cost', 'Control', n_splits=100, ml_model='catboost', binary_cols=binary_cols, all_y_list=all_y_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bdce400",
   "metadata": {},
   "source": [
    "all_y_list = [\"own_value_ls\",\"totalincome_1mo\",\"elset_sh_eq\",\"tot_exp_2\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d8b647",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdef339d",
   "metadata": {},
   "source": [
    "### Plot the distribution of S(Z) (Median S(Z) per row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcebca9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(s_medians_per_row,bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d5b5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.ticker as ticker\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(s_medians_per_row, bins=100)\n",
    "\n",
    "# Format x-axis to use 'k' notation\n",
    "plt.gca().xaxis.set_major_formatter(ticker.FuncFormatter(lambda x, p: f'{x/1000:.0f}k' if x >= 1000 else f'{x:.0f}'))\n",
    "\n",
    "# Manually set the tick positions for even distribution\n",
    "x_min, x_max = plt.xlim()\n",
    "custom_ticks = np.linspace(x_min, x_max, 30)  # 15 ticks across the range\n",
    "plt.xticks(custom_ticks, rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd5b483",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "sns.kdeplot(s_medians_per_row,shade=True)\n",
    "# Format x-axis to use 'k' notation\n",
    "plt.gca().xaxis.set_major_formatter(ticker.FuncFormatter(lambda x, p: f'{x/1000:.0f}k' if x >= 1000 else f'{x:.0f}'))\n",
    "\n",
    "# Manually set the tick positions for even distribution\n",
    "x_min, x_max = plt.xlim()\n",
    "custom_ticks = np.linspace(x_min, x_max, 30)  # 15 ticks across the range\n",
    "plt.xticks(custom_ticks, rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a14309",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df_main['own_value_ls'], bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05de23c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main['own_value_ls'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b114b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_median_dict = {'s_median': s_medians_per_row}\n",
    "s_median_df = pd.DataFrame(s_median_dict)\n",
    "s_median_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df79b3b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47622419",
   "metadata": {},
   "source": [
    "## **First let's Compare the models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70008fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_lambda_blp = np.median(models_comparisons['small_lambda_blp'])\n",
    "small_lambda_gates = np.median(models_comparisons['small_lambda_gates'])\n",
    "print(F\"small_lambda_blp: {small_lambda_blp.round(0)}, small_lambda_gates: {small_lambda_gates.round(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986b1419",
   "metadata": {},
   "source": [
    "NN: small_lambda_blp: 51879186.868425295, small_lambda_gates: 188142249.3749832\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e493cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_lambda_blp_normalized = np.median(models_comparisons['small_lambda_blp']) / df_main['own_value_ls'].var()\n",
    "small_lambda_gates_normalized = np.median(models_comparisons['small_lambda_gates']) / df_main['own_value_ls'].var()\n",
    "print(F\"small_lambda_blp_normalized: {small_lambda_blp_normalized}, small_lambda_gates_normalized: {small_lambda_gates_normalized}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41bb5872",
   "metadata": {},
   "source": [
    "- elasticnet: lambda_blp_normalized: 0.15897482512187378, lambda_gates_normalized: 0.5596844870804755\n",
    "- xgboost: lambda_blp_normalized: 0.10458710928863613, lambda_gates_normalized: 0.49125177179907553\n",
    "- Random Forest: lambda_blp_normalized: 0.243244078015595, lambda_gates_normalized: 0.5830742598858208\n",
    "- catboost_100: lambda_blp_normalized: 0.21525394117555752, lambda_gates_normalized: 0.4747823367888713\n",
    "- NN: lambda_blp_normalized: 0.08692043861869532, lambda_gates_normalized: 0.388215865677688"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248fa33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main['own_value_ls'].var()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79335c0f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1aecc01",
   "metadata": {},
   "source": [
    "## **Second: For the best model, extract the results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be66ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691374b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for run_i, run in enumerate(results['blp'], start=1):\n",
    "    for d in run:\n",
    "        if 'name' in d:  # only coefficient rows\n",
    "            rows.append({\n",
    "                'run': run_i,\n",
    "                'name': d['name'],\n",
    "                'coef': float(d['coef']),\n",
    "                'std_err': float(d['std_err']),\n",
    "                'z': float(d['z']),\n",
    "                'p': float(d['p']),\n",
    "                'ci_low': float(d['ci_low']),\n",
    "                'ci_high': float(d['ci_high']),\n",
    "            })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "term = 'D-p_S(Z)_shifted'\n",
    "s = df[df['name'] == term].set_index('run')['coef']\n",
    "median_val = s.median()\n",
    "run_with_median = (s - median_val).abs().idxmin()\n",
    "\n",
    "blp_median_coef = (df[df['run'] == run_with_median]\n",
    "       .reset_index(drop=True))\n",
    "\n",
    "blp_median_coef \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92517e9",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fbf047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Flatten `results['gates']` (one row per split \u00d7 group) -----\n",
    "rows = []\n",
    "for run in results['gates']:\n",
    "    for d in run:\n",
    "        rows.append({\n",
    "            \"group\": int(d[\"group\"]),\n",
    "            \"gamma\": float(d[\"gamma\"]),\n",
    "            \"std_err\": float(d[\"std_err\"]),\n",
    "            \"ci_low\": float(d[\"ci_low\"]),\n",
    "            \"ci_high\": float(d[\"ci_high\"]),\n",
    "            # if your dict already has \"p\", we'll keep it; else we compute below\n",
    "            \"p\": float(d[\"p\"]) if \"p\" in d else np.nan,\n",
    "        })\n",
    "\n",
    "gdf = pd.DataFrame(rows)\n",
    "\n",
    "# If p-values were not stored per split, approximate from gamma/std_err (two-sided normal)\n",
    "if gdf[\"p\"].isna().any():\n",
    "    z = gdf[\"gamma\"] / gdf[\"std_err\"]\n",
    "    gdf[\"p\"] = 2 * (1 - norm.cdf(np.abs(z)))\n",
    "\n",
    "# ----- Aggregate across splits EXACTLY like the R code (medians by group) -----\n",
    "gates_summary = (\n",
    "    gdf.groupby(\"group\", as_index=False)\n",
    "       .agg(gamma=(\"gamma\", \"median\"),\n",
    "       std_err=(\"std_err\", \"median\"),\n",
    "       ci_low=(\"ci_low\", \"median\"),\n",
    "       ci_high=(\"ci_high\", \"median\"),\n",
    "       p=(\"p\", \"median\"),\n",
    "       )\n",
    "\n",
    ")\n",
    "\n",
    "gates_summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1675cf",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f9ab81",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for run_i, run in enumerate(results['clan'], start=1):\n",
    "    for d in run:\n",
    "        r = {'run': run_i}\n",
    "        r.update(d)\n",
    "        rows.append(r)\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "diff_col = 'Diff (G5 - G1)'\n",
    "\n",
    "# drop rows with NaN diff (so medians/closest work cleanly)\n",
    "df_ = df.dropna(subset=[diff_col]).copy()\n",
    "\n",
    "# median diff per covariate\n",
    "med = df_.groupby('covariate')[diff_col].median().rename('diff_median')\n",
    "\n",
    "# find the row per covariate closest to its median\n",
    "tmp = df_.merge(med, on='covariate', how='left')\n",
    "idx = (tmp[diff_col] - tmp['diff_median']).abs().groupby(tmp['covariate']).idxmin()\n",
    "\n",
    "clan = (tmp.loc[idx]\n",
    "       .drop(columns=['diff_median'])\n",
    "       .sort_values('covariate')\n",
    "       .reset_index(drop=True))\n",
    "\n",
    "clan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d220c276",
   "metadata": {},
   "outputs": [],
   "source": [
    "#additional_feature_drop = ['vehicle_motorcycle_e','vehicle_toktok_e','vehicle_tricycle_e','skip_meal_e','has_heater_e','has_fridge_e','cart_e','bicycle_e']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3175ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "clan[clan['p (two-sided)'] < 0.05].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe94c649",
   "metadata": {},
   "outputs": [],
   "source": [
    "clan[clan['p (two-sided)'] < 0.05]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}